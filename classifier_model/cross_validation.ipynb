{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c96ca53",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a3ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from cnn_2d_model.CNN_LSTM_model import HelplessnessClassifier as CNN_LSTM_Classifier\n",
    "from cnn_3d_model.model import HelplessnessClassifier as CNN_3D_Classifier\n",
    "from pre_trained_transformer_model.model import create_swin3d_t_model_training\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps' # Apple Silicon \n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' # Nvidia GPU\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ea243",
   "metadata": {},
   "source": [
    "# Custom dataset for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c15c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelplessnessVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, train_transform=None, test_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "        self.train = True\n",
    "        self.video_folders = []\n",
    "\n",
    "        # Read all video sequences in the extracted frames folders\n",
    "        categories = ['extreme-helpless', 'little_helplessness', 'no-helpless']\n",
    "        for category in categories:\n",
    "            category_dir = os.path.join(root_dir, category)\n",
    "            if not os.path.exists(category_dir):\n",
    "                print(f\"Warning: Category folder {category_dir} does not exist.\")\n",
    "                continue\n",
    "            for video_folder in sorted(os.listdir(category_dir)):\n",
    "                video_path = os.path.join(category_dir, video_folder)\n",
    "                if os.path.isdir(video_path):\n",
    "                    self.video_folders.append(video_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_folders)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_folders[index]\n",
    "\n",
    "        # Each video is a sequence of frames, so need to get each frame\n",
    "        frame_files = sorted(os.listdir(video_path))\n",
    "        sequence = []\n",
    "        random_state = torch.get_rng_state()\n",
    "        for frame_name in frame_files:\n",
    "            frame_path = os.path.join(video_path, frame_name)\n",
    "            frame = Image.open(frame_path)\n",
    "            if self.train_transform and self.test_transform:\n",
    "                # To allow augmentation, we need to apply the same \"random\" transformation to each frame\n",
    "                torch.set_rng_state(random_state)\n",
    "                frame = self.train_transform(frame) if self.train else self.test_transform(frame)\n",
    "            sequence.append(frame)\n",
    "\n",
    "        # Convert the sequence from list of tensors to (sequence_length, channels, height, width) tensor\n",
    "        sequence = torch.stack(sequence)\n",
    "        sequence = torch.transpose(sequence, 0, 1)  # REMOVE THIS IF YOU NEED THE SEQUENCE_LENGTH AND CHANNELS DIMENSIONS SWITCHED!\n",
    "\n",
    "        # Retrieve the level of helplessness label from path of video\n",
    "        split_path = video_path.split(os.sep)  # Changed '/' to os.sep for cross-platform compatibility\n",
    "\n",
    "        # Ensure that split_path has at least 2 components\n",
    "        if len(split_path) >= 2:\n",
    "            category = split_path[-2]  # category folder is second last in path\n",
    "        else:\n",
    "            raise ValueError(f\"Path structure issue: {video_path}\")\n",
    "\n",
    "        label = -1\n",
    "        if category == 'no-helpless':\n",
    "            label = 0\n",
    "        elif category == 'little_helplessness':\n",
    "            label = 1\n",
    "        elif category == 'extreme-helpless':\n",
    "            label = 2\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5590c8",
   "metadata": {},
   "source": [
    "# 2D CNN-LSTM 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a5a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transforms and dataset for 2D CNN-LSTM\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((112, 112), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),  # converts PIL Image in L mode to tensor shape (1, H, W) with pixel values [0,1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = HelplessnessVideoDataset('../data/train', train_transform=train_transform, test_transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737a9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:01<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 45.10%, Fold Accuracy: 45.45%\n",
      "\n",
      "Fold 2\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:05<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 31.81%, Fold Accuracy: 34.38%\n",
      "\n",
      "Fold 3\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 31.34%, Fold Accuracy: 34.38%\n",
      "\n",
      "Fold 4\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 44.82%, Fold Accuracy: 46.88%\n",
      "\n",
      "Fold 5\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 45.45%, Fold Accuracy: 46.88%\n",
      "\n",
      "Average F1 Score across all folds: 39.71%\n",
      "Average Accuracy across all folds: 41.59%\n"
     ]
    }
   ],
   "source": [
    "# to do 5-fold cross validation, we are using the approach shown here: https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
    "\n",
    "# Initialize how many folds we want for cross-validation\n",
    "# Using KFold from scikit-learn for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "average_f1score = 0\n",
    "average_accuracy = 0\n",
    "\n",
    "# Loop through each fold, training the model and reporting results\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('------')\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the model, loss criterion, optimizer and number of epoches\n",
    "    model = CNN_LSTM_Classifier(num_classes=3).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Train the model for the given epoches on the train_loader for this fold\n",
    "    for epochs in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_dataset.train = True # ensure train transforms on used on the data\n",
    "\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "            sequences = torch.transpose(sequences, 1, 2) # need to transpose the sequence length and channel dimensions\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test_loader for this fold\n",
    "    model.eval()\n",
    "    train_dataset.train = False # ensure test transforms on used on the data\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            sequences = torch.transpose(sequences, 1, 2) # need to transpose the sequence length and channel dimensions\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store predictions and ground truth labels in lists\n",
    "            predictions.extend(predicted.detach().cpu().tolist())\n",
    "            ground_truth.extend(labels.detach().cpu().tolist())\n",
    "    \n",
    "    # Using scikit-learn, print the F1 score of this fold: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    from sklearn.metrics import f1_score\n",
    "    fold_f1score = f1_score(ground_truth, predictions, average='weighted') * 100\n",
    "    fold_accuracy = 100 * correct / total\n",
    "    print(f'Fold F1 Score: {fold_f1score:.2f}%, Fold Accuracy: {fold_accuracy:.2f}%\\n')\n",
    "    average_f1score += fold_f1score\n",
    "    average_accuracy += fold_accuracy\n",
    "\n",
    "average_f1score /= 5\n",
    "average_accuracy /= 5\n",
    "print(f'Average F1 Score across all folds: {average_f1score:.2f}%')\n",
    "print(f'Average Accuracy across all folds: {average_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a917c",
   "metadata": {},
   "source": [
    "# 3D CNN 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc7d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transforms and dataset for 3D CNN\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation((-45.0, 45.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.41500069, 0.36530493, 0.33830512], [0.29042152, 0.27499218, 0.27738131])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.41500069, 0.36530493, 0.33830512], [0.29042152, 0.27499218, 0.27738131])\n",
    "])\n",
    "\n",
    "train_dataset = HelplessnessVideoDataset('../data/train', train_transform=train_transform, test_transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ccc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [35:12<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 54.57%, Fold Accuracy: 54.55%\n",
      "\n",
      "Fold 2\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [35:58<00:00, 15.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 49.17%, Fold Accuracy: 50.00%\n",
      "\n",
      "Fold 3\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [35:57<00:00, 15.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 66.12%, Fold Accuracy: 65.62%\n",
      "\n",
      "Fold 4\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [35:25<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 40.20%, Fold Accuracy: 40.62%\n",
      "\n",
      "Fold 5\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [33:55<00:00, 15.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 61.60%, Fold Accuracy: 62.50%\n",
      "\n",
      "Average F1 Score across all folds: 54.33%\n",
      "Average Accuracy across all folds: 54.66%\n"
     ]
    }
   ],
   "source": [
    "# to do 5-fold cross validation, we are using the approach shown here: https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
    "\n",
    "# Initialize how many folds we want for cross-validation\n",
    "# Using KFold from scikit-learn for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "average_f1score = 0\n",
    "average_accuracy = 0\n",
    "\n",
    "# Loop through each fold, training the model and reporting results\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('------')\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=2,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=2,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the model, loss criterion, optimizer and number of epoches\n",
    "    model = CNN_3D_Classifier(input_channels=3).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    num_epochs = 135\n",
    "\n",
    "    # Train the model for the given epoches on the train_loader for this fold\n",
    "    for epochs in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_dataset.train = True # ensure train transforms on used on the data\n",
    "\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test_loader for this fold\n",
    "    model.eval()\n",
    "    train_dataset.train = False # ensure test transforms on used on the data\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store predictions and ground truth labels in lists\n",
    "            predictions.extend(predicted.detach().cpu().tolist())\n",
    "            ground_truth.extend(labels.detach().cpu().tolist())\n",
    "    \n",
    "    # Using scikit-learn, print the F1 score of this fold: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    from sklearn.metrics import f1_score\n",
    "    fold_f1score = f1_score(ground_truth, predictions, average='weighted') * 100\n",
    "    fold_accuracy = 100 * correct / total\n",
    "    print(f'Fold F1 Score: {fold_f1score:.2f}%, Fold Accuracy: {fold_accuracy:.2f}%\\n')\n",
    "    average_f1score += fold_f1score\n",
    "    average_accuracy += fold_accuracy\n",
    "\n",
    "average_f1score /= 5\n",
    "average_accuracy /= 5\n",
    "print(f'Average F1 Score across all folds: {average_f1score:.2f}%')\n",
    "print(f'Average Accuracy across all folds: {average_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c70bbe",
   "metadata": {},
   "source": [
    "# Pre-trained Video Swin Transformer Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8062f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transforms and dataset for 3D CNN\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation((-45.0, 45.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.41500069, 0.36530493, 0.33830512], [0.29042152, 0.27499218, 0.27738131])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.41500069, 0.36530493, 0.33830512], [0.29042152, 0.27499218, 0.27738131])\n",
    "])\n",
    "\n",
    "train_dataset = HelplessnessVideoDataset('../data/train', train_transform=train_transform, test_transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e921e412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:14<00:00, 37.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 68.79%, Fold Accuracy: 69.70%\n",
      "\n",
      "Fold 2\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:17<00:00, 37.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 63.12%, Fold Accuracy: 65.62%\n",
      "\n",
      "Fold 3\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:07<00:00, 36.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 83.93%, Fold Accuracy: 84.38%\n",
      "\n",
      "Fold 4\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:08<00:00, 36.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 63.59%, Fold Accuracy: 68.75%\n",
      "\n",
      "Fold 5\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:05<00:00, 36.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold F1 Score: 74.84%, Fold Accuracy: 75.00%\n",
      "\n",
      "Average F1 Score across all folds: 70.85%\n",
      "Average Accuracy across all folds: 72.69%\n"
     ]
    }
   ],
   "source": [
    "# to do 5-fold cross validation, we are using the approach shown here: https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
    "\n",
    "# Initialize how many folds we want for cross-validation\n",
    "# Using KFold from scikit-learn for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "average_f1score = 0\n",
    "average_accuracy = 0\n",
    "\n",
    "# Loop through each fold, training the model and reporting results\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('------')\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=1,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=1,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_index),\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the model, loss criterion, optimizer and number of epoches\n",
    "    model = create_swin3d_t_model_training().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Train the model for the given epoches on the train_loader for this fold\n",
    "    for epochs in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_dataset.train = True # ensure train transforms on used on the data\n",
    "\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test_loader for this fold\n",
    "    model.eval()\n",
    "    train_dataset.train = False # ensure test transforms on used on the data\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store predictions and ground truth labels in lists\n",
    "            predictions.extend(predicted.detach().cpu().tolist())\n",
    "            ground_truth.extend(labels.detach().cpu().tolist())\n",
    "    \n",
    "    # Using scikit-learn, print the F1 score of this fold: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    from sklearn.metrics import f1_score\n",
    "    fold_f1score = f1_score(ground_truth, predictions, average='weighted') * 100\n",
    "    fold_accuracy = 100 * correct / total\n",
    "    print(f'Fold F1 Score: {fold_f1score:.2f}%, Fold Accuracy: {fold_accuracy:.2f}%\\n')\n",
    "    average_f1score += fold_f1score\n",
    "    average_accuracy += fold_accuracy\n",
    "\n",
    "average_f1score /= 5\n",
    "average_accuracy /= 5\n",
    "print(f'Average F1 Score across all folds: {average_f1score:.2f}%')\n",
    "print(f'Average Accuracy across all folds: {average_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpt419project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
